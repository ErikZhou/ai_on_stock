{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhoueric/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:121: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1611, 256, 128, 1)\n",
      "(378, 256, 128, 1)\n",
      "[0.4920634922211763, 0.5079365372657776]\n",
      "Test score: 0.4920634922211763\n",
      "Test accuracy: 0.5079365372657776\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "#用每日新闻预测金融市场变化\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from datetime import date\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from sklearn.svm import SVR\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import cross_val_score  #做交叉验证\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D,MaxPooling2D\n",
    "from keras.layers.core import Dense,Dropout,Activation,Flatten\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "\n",
    "#读取数据划分训练集和测试集\n",
    "data=pd.read_csv('./Combined_News_DJIA.csv')\n",
    "train=data[data['Date']<'2015-01-01']\n",
    "test=data[data['Date']>'2014-12-31']\n",
    "\n",
    "#构建语料库\n",
    "Corpus=data[data.columns[2:]]\n",
    "corpus=Corpus.values.flatten().astype(str)\n",
    "\n",
    "#训练集和测试集\n",
    "X_train=train[train.columns[2:]]\n",
    "X_train=X_train.values.astype(str)\n",
    "X_train=np.array([' '.join(x) for x in X_train])\n",
    "\n",
    "X_test=test[test.columns[2:]]\n",
    "X_test=X_test.values.astype(str)\n",
    "X_test=np.array([' '.join(x) for x in X_test])\n",
    "\n",
    "#训练标签和测试标签\n",
    "y_train=train['Label'].values\n",
    "y_test=test['Label'].values\n",
    "\n",
    "#预料、训练集、测试集分词处理\n",
    "corpus=[word_tokenize(x) for x in corpus]\n",
    "X_train=[word_tokenize(x) for x in X_train]\n",
    "X_test=[word_tokenize(x) for x in X_test]\n",
    "\n",
    "#数据集预处理\n",
    "stop_words=stopwords.words('english')  #停用词\n",
    "wordnet_lemmatizer=WordNetLemmatizer()  #词性还原\n",
    "def hasNumber(inputString):      #数字\n",
    "    return bool(re.search('\\d',inputString))\n",
    "def isSymbol(inputString):     #特殊字符\n",
    "    return bool(re.match('[^\\w]',inputString))\n",
    "def check(word):  #如果需要这个单词，则True，如果应该去除，则False\n",
    "    word=word.lower()\n",
    "    if word in stop_words:\n",
    "        return False\n",
    "    elif hasNumber(word) or isSymbol(word):\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "def preprocessing(sen):   #数据预处理\n",
    "    res=[]\n",
    "    for word in sen:\n",
    "        if check(word):\n",
    "            word=word.lower().replace(\"b'\",'').replace('b\"','').replace('\"','').replace(\"'\",'')\n",
    "            res.append(wordnet_lemmatizer.lemmatize(word))\n",
    "    return res\n",
    "\n",
    "#对语料库，训练集，测试集进行预处理\n",
    "corpus=[preprocessing(x) for x in corpus]\n",
    "X_train=[preprocessing(x) for x in X_train]\n",
    "X_test=[preprocessing(x) for x in X_test]\n",
    "\n",
    "#构建Word2Vec词向量模型\n",
    "model=Word2Vec(corpus,size=128,window=5,min_count=5,workers=4)\n",
    "#获得模型的所有词汇\n",
    "vocab=model.wv.vocab\n",
    "#得到任意text的vector\n",
    "def get_vector(word_list):\n",
    "    res=np.zeros([128])\n",
    "    count=0\n",
    "    for word in word_list:\n",
    "        if word in vocab:\n",
    "            res += model[word]\n",
    "            count += 1\n",
    "    return res/count\n",
    "\n",
    "wordlist_train=X_train\n",
    "wordlist_test=X_test\n",
    "\n",
    "'''\n",
    "#获得训练集和测试集的文本向量\n",
    "X_train=[get_vector(x) for x in X_train]\n",
    "X_test=[get_vector(x) for x in X_test]\n",
    "#建立SVM模型\n",
    "params=[0.1,0.5,1,3,5,7,10,12,16,20,25,30,35,40]\n",
    "test_cores=[]\n",
    "for param in params:\n",
    "    clf=SVR(gamma=param)\n",
    "    test_core=cross_val_score(clf,X_train,y_train,cv=3,scoring='roc_auc')\n",
    "    test_cores.append(np.mean(test_core))\n",
    "print(test_cores)\n",
    "plt.plot(params,test_cores)\n",
    "plt.title(\"Param vs CV AUC Score\")\n",
    "plt.show()\n",
    "'''\n",
    "\n",
    "#对于每天的新闻，我们会考虑前256个单词，不够的我们用[0000000]补上\n",
    "def transform_to_matrix(x,padding_size=256,vec_size=128):\n",
    "    res=[]\n",
    "    for sen in x:\n",
    "        matrix=[]\n",
    "        for i in range(padding_size):\n",
    "            try:\n",
    "                matrix.append(model[sen[i]].tolist())\n",
    "            except: #这个单词找不到或者sen没那么长，直接用全为0的vector表示\n",
    "                matrix.append([0]*vec_size)\n",
    "        res.append(matrix)\n",
    "    return res\n",
    "X_train=transform_to_matrix(wordlist_train)\n",
    "X_test=transform_to_matrix(wordlist_test)\n",
    "#print(X_train[123])\n",
    "X_train=np.array(X_train)\n",
    "X_test=np.array(X_test)\n",
    "#print(X_train.shape)\n",
    "#print(X_test.shape)\n",
    "X_train=X_train.reshape(X_train.shape[0],X_train.shape[1],X_train.shape[2],1)\n",
    "X_test=X_test.reshape(X_test.shape[0],X_test.shape[1],X_test.shape[2],1)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "\n",
    "#设置参数\n",
    "batch_size=32\n",
    "n_filter=16\n",
    "filter_length=4\n",
    "nb_epoch=5\n",
    "n_pool=2\n",
    "#新建一个Sequential的模型\n",
    "model=Sequential()\n",
    "model.add(Conv2D(n_filter,(filter_length,filter_length),input_shape=(256,128,1)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(n_filter,(filter_length,filter_length)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(n_pool,n_pool)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='mse',optimizer='adadelta',metrics=['accuracy'])\n",
    "model.fit(X_train,y_train,batch_size=batch_size,epochs=nb_epoch,verbose=0)\n",
    "score=model.evaluate(X_test,y_test,verbose=0)\n",
    "print(score)\n",
    "print('Test score:',score[0])\n",
    "print('Test accuracy:',score[1])\n",
    "\n",
    "#Test score: 0.4920634922211763\n",
    "#Test accuracy: 0.5079365372657776"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
