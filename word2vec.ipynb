{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-4-d867418866c8>, line 104)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-d867418866c8>\"\u001b[0;36m, line \u001b[0;32m104\u001b[0m\n\u001b[0;31m    params = [0.1,0.5,1,3,5,7,10,12,16,20,25,30,35,40]\u001b[0m\n\u001b[0m                                                      ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from datetime import date\n",
    "import os\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#导入数据\n",
    "#####导入数据######\n",
    "#os.chdir(r'D:/.../.../利用每日新闻预测金融市场变化')\n",
    "data = pd.read_csv('./Combined_News_DJIA.csv')\n",
    "\n",
    "########分割测试/训练集\n",
    "train = data[data['Date'] < '2015-01-01']\n",
    "test = data[data['Date'] > '2014-12-31']\n",
    "\n",
    "#把每条新闻做成一个单独的句子\n",
    "X_train = train[train.columns[2:]]\n",
    "corpus = X_train.values.flatten().astype(str)\n",
    "\n",
    "X_train = X_train.values.astype(str)\n",
    "X_train = np.array([' '.join(x) for x in X_train])\n",
    "X_test = test[test.columns[2:]]\n",
    "X_test = X_test.values.astype(str)\n",
    "X_test = np.array([' '.join(x) for x in X_test])\n",
    "y_train = train['Label'].values\n",
    "y_test = test['Label'].values\n",
    "\n",
    "将每个单词给分隔开\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "corpus = [word_tokenize(x) for x in corpus]\n",
    "X_train = [word_tokenize(x) for x in X_train]\n",
    "X_test = [word_tokenize(x) for x in X_test] \n",
    "预处理\n",
    "#预处理\n",
    "#小写化\n",
    "#删除停用词\n",
    "#删除数字与符号\n",
    "#lemma\n",
    "# 停止词\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "# 数字\n",
    "import re\n",
    "def hasNumbers(inputString):\n",
    "    return bool(re.search(r'\\d', inputString))\n",
    "\n",
    "# 特殊符号\n",
    "def isSymbol(inputString):\n",
    "    return bool(re.match(r'[^\\w]', inputString))\n",
    "\n",
    "# lemma\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def check(word):\n",
    "    \"\"\"\n",
    "    如果需要这个单词，则True\n",
    "    如果应该去除，则False\n",
    "    \"\"\"\n",
    "    word= word.lower()\n",
    "    if word in stop:\n",
    "        return False\n",
    "    elif hasNumbers(word) or isSymbol(word):\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "# 把上面的方法综合起来\n",
    "def preprocessing(sen):\n",
    "    res = []\n",
    "    for word in sen:\n",
    "        if check(word):\n",
    "            # 这一段的用处仅仅是去除python里面byte存str时候留下的标识。。之前数据没处理好，其他case里不会有这个情况\n",
    "            word = word.lower().replace(\"b'\", '').replace('b\"', '').replace('\"', '').replace(\"'\", '')\n",
    "            res.append(wordnet_lemmatizer.lemmatize(word))\n",
    "    return res            \n",
    "             \n",
    "#将三个数据组进行预处理\n",
    "corpus = [preprocessing(x) for x in corpus]\n",
    "X_train = [preprocessing(x) for x in X_train]\n",
    "X_test = [preprocessing(x) for x in X_test] \n",
    "#训练NLP模型\n",
    "\n",
    "#训练NLP模型\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "model = Word2Vec(corpus, size=128, window=5, min_count=5, workers=4) \n",
    "          \n",
    "vocab = model.wv.vocab\n",
    "\n",
    "# 得到任意text的vector\n",
    "def get_vector(word_list):\n",
    "    # 建立一个全是0的array\n",
    "    res =np.zeros([128])\n",
    "    count = 0\n",
    "    for word in word_list:\n",
    "        if word in vocab:\n",
    "\n",
    "\n",
    "params = [0.1,0.5,1,3,5,7,10,12,16,20,25,30,35,40]\n",
    "\n",
    "test_scores = []\n",
    "for param in params:\n",
    "    clf = SVR(gamma=param)\n",
    "    test_score = cross_val_score(clf, X_train, y_train, cv=3, scoring='roc_auc')\n",
    "    test_scores.append(np.mean(test_score))\n",
    "             \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(params, test_scores)\n",
    "plt.title(\"Param vs CV AUC Score\");  \n",
    "\n",
    "res += model[word] \n",
    "count += 1 \n",
    "return res/count \n",
    "\n",
    "wordlist_train = X_train\n",
    "wordlist_test = X_test\n",
    "X_train = [get_vector(x) for x in X_train]\n",
    "X_test = [get_vector(x) for x in X_test] \n",
    "\n",
    "#建立ML模型\n",
    "\n",
    "\n",
    "params = [0.1,0.5,1,3,5,7,10,12,16,20,25,30,35,40]\n",
    "test_scores = []\n",
    "for param in params:\n",
    "    clf = SVR(gamma=param)\n",
    "    test_score = cross_val_score(clf, X_train, y_train, cv=3, scoring='roc_auc')\n",
    "    test_scores.append(np.mean(test_score))\n",
    "             \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(params, test_scores)\n",
    "plt.title(\"Param vs CV AUC Score\");  \n",
    "#使用CNN提升逼格\n",
    "########利用CNN提升逼格\n",
    "# 用vector表示出一个大matrix，并用CNN做“降维+注意力”\n",
    "def transform_to_matrix(x, padding_size=256, vec_size=128):\n",
    "    res = []\n",
    "    for sen in x:\n",
    "        matrix = []\n",
    "        for i in range(padding_size):\n",
    "            try:\n",
    "                matrix.append(model[sen[i]].tolist())\n",
    "            except:\n",
    "                # 这里有两种except情况，\n",
    "                # 1. 这个单词找不到\n",
    "                # 2. sen没那么长\n",
    "                # 不管哪种情况，我们直接贴上全是0的vec\n",
    "                matrix.append([0] * vec_size)\n",
    "        res.append(matrix)\n",
    "    return res\n",
    "\n",
    "\n",
    "X_train = transform_to_matrix(wordlist_train)\n",
    "X_test = transform_to_matrix(wordlist_test)\n",
    "\n",
    "print(X_train[123])\n",
    "\n",
    "#变成np的数组，便于处理\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "\n",
    "#查看数组的大小\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], 1, X_train.shape[1], X_train.shape[2])\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, X_test.shape[1], X_test.shape[2])\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "####定义cnn模型\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D,MaxPooling2D\n",
    "from keras.layers.core import Dense,Dropout,Activation,Flatten\n",
    "#设置参数\n",
    "batch_size =32\n",
    "n_filter = 16\n",
    "filter_length = 4\n",
    "nb_epoch = 5\n",
    "n_pool = 2\n",
    "\n",
    "#新建一个sequential的模型\n",
    "model = Sequential()\n",
    "model.add(Convolution2D(n_filter,filter_length,filter_length,input_shape=(1, 256, 128)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(n_filter,filter_length,filter_length))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(n_pool, n_pool)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "\n",
    "# 后面接上一个ANN\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('softmax'))\n",
    "# compile模型\n",
    "model.compile(loss='mse',optimizer='adadelta',metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch,\n",
    "          verbose=0)\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
